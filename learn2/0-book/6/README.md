# 六、深度学习用于文本和序列
--
本章包括以下内容：

* 将文本数据处理为有用的数据表示
* 使用循环神经网络
* 使用一维卷积神经网络处理序列

本章将介绍使用深度学习模型处理**文本序列**、**时间序列**、**一般序列**数据<br>
用于处理序列的两种基本的深度学习算法：

* 循环神经网络（recurrent neural network）
* 一维卷积神经网络（1D convnet）（已经大概能想到）

主要应用：

* 文本分类和时间序列分类，比如识别文章的主题或书的作者
* 时间序列对比，比如估测两个文档或两支股票行情的相关程度
* 序列到序列的学习，比如将英语翻译成法语
* 情感分析，比如将推文或电影评论的情感划分为正面或负面
* 时间序列预测，比如根据某地最近的天气数据来预测未来天气

## 6.1 处理文本数据
**深度学习用于自然语言处理是将模式识别应用于单词、句子和段落，这与计算机视觉是将模式识别应用于像素大致相同**<br>

文本向量化(vectorize)是指将文本转换为数值张量的过程。它有多种实现方法：

* 将文本分割为单词，并将每个单词转换为一个向量（1D张量）
* 将文本分割为字符，并将每个字符转换为一个向量
* 提取单词或字符的n-gram，并将每个n-gram转换为一个向量。n-gram是多个连续单词或字符的集合（词袋）

**标记(token)**：将文本分解而成的单元（单词、字符、g-gram）<br>
**分词(tokenization)**：将文本分解成标记的过程<br>
**文本向量化**：文本--*(分词)*-->标记-->*(关联)*-->数值向量
![](./images/6.1-1.png)

分词方法有很多，关联方法也有很多，两种主要**关联方法**：

* one-hot编码(one-hotencoding)
* 标记嵌入[token embedding,通常只用于单词，叫作**词嵌入(word embedding)**]

### 6.1.1 单词和字符的one-hot编码
每个单词与一个唯一的整数索引相关联，如果将这个整数索引 i 转换为长度为N的二进制向量（N是词表大小），这个向量只有第i个元素是1，其余元素都为0
[one-hot编码](./book6_1.py)

### 6.1.2 使用词嵌入
![](./images/6.1-2.png)

获取词嵌入有两种方法：

1. **利用Embedding层学习词嵌入**：在完成主任务(比如文档分类或情感预测)的同时学习词嵌入。在这种情况下，一开始是随机的词向量，然后对这些词向量进行学习，其学习方式与学习神经网络的权重相同。
	
	词嵌入的作用应该是将人类的语言映射到几何空间中
	![](./images/6.1-3.png)
	从 cat 到 tiger 的向量与从 dog 到 wolf 的向量相等，这个向量可以被解释为“从宠物到野生动物”向量。同样，从 dog 到 cat 的向量与从 wolf 到 tiger 的向量也相等，它可以被解释为“从犬科到猫科”向量。<br>
	但没有一个完美的词嵌入空间能完全映射人类语言，所以，合理的做法是对每个新任务都学习一个新的嵌入空间。反向传播让这种学习变得很简单，keras更简单。我们要做的就是学习一个权重层Embedding层。<br>
	最好将 Embedding 层理解为一个字典，将整数索引(表示特定单词)映射为密集向量。它接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量。Embedding层实际上是一种字典查找<br>
	单词索引 ----> Embedding层 ----> 对应的词向量<br>
	Embedding层的输入是一个二维整数张量，其形状为(samples,sequence\_length)，返回(samples,sequence\_length,embedding\_dimensionality)<br>
	[在IMDB数据上使用Embedding层和分类器,line=29](./book6_1-2.py)，将整数索引(25000,20)映射为密集向量(25000,20,8)<br>
	
2. **预训练词嵌入**：在不同于待解决问题的机器学习任务上预计算好词嵌入，然后将其加载到模型中（👻：类似卷积神经网络使用预训练的模型），可以使用word2vec或GloVe。👻：不知有无中文模型

### 6.1.3 整合在一起:从原始文本到词嵌入
与[book6_1-2.py](./book6_1-2.py)类似：将句子嵌入到向量序列中，然后展平，最后加一个Dense层。但此处使用预训练的词嵌入。此外还使用IMDB原始文本，不使用Keras内置的已经预分词的IMDB数据

1. [下载IMDB数据](http://mng.bz/0tIo)的原始文本，并进行处理
2. 对数据进行分词
3. [下载GloVe词嵌入](https://nlp.stanford.edu/projects/glove/)
4. [整合在一起](./book6_1-3.py)
![](./images/6.1-4.png)
<center>使用词嵌入</center>
可以在不加载预训练词嵌入、也不冻结嵌入层的情况下训练相同的模型。在这种情况下，你将会学到针对任务的输入标记的嵌入。如果有大量的可用数据，这种方法通常比预训练词嵌入更加强大
![](./images/6.1-5.png)
<center>不使用词嵌入</center>

### 6.1.4 小结
现在已经学会了下列内容：

* 将原始文本转换为神经网络能够处理的格式
* 使用Keras模型的Embedding层来学习针对特定任务的标记嵌入
* 使用预训练词嵌入在小型自然语言处理问题上获得额外的性能提升

## 6.2 理解循环神经网络
之前的网络：密集连接网络和卷积神经网络都没有记忆性。它们单独的处理每个输入，在输入与输入之间没有保存任何状态。这样的网络想要处理序列，只能向网络展示整个序列，一次性处理。这种网络叫**前馈网络(feedforward network)**<br>
生物智能以渐进的方式处理信息，同时保存一个关于所处理内容的内部模型，这个模型是根据过去的信息构建的，并随着新信息的进入而不断更新<br>
**循环神经网络**采用同样的原理：遍历所有序列，并保存一个**状态**(state)，其中包含与已查看内容相关的信息。<br>
![](./images/6.2-1.png)
**<center>将前一次的计算输出迭代入到网络中</center>**<br>
一条评论，即一个序列，序列的前一个元素，经过网络后，得到的输出迭代入网络中，下一个元素来的时候，网络中已经有了前所有元素的信息<br>

[一个简单的RNN](./book6_2-1.py)
![](./images/6.2-2.png)

### 6.2.1 Keras 中的循环层
```
from keras.layers import SimpleRNN
```
与Numpy实现的RNN有个小区别：

* **SimpleRNN**层能够像其他Keras层一样处理序列批量```
(batch_size, timesteps, input_features)
```

* Numpy示例处理：
```
(timesteps, input_features)
```

**SimpleRNN** 两种运行模式：

* ```return_sequences=False```：返回每个输入序列的最终输出```(batch_size, output_features)```(默认)，每一个序列，都只看最后一步的输出

* ```return_sequences=True```：返回每个时间步连续输出的完整序列```(batch_size, timesteps, output_features)```每一个序列，都看完整过程输出
	
	`batch_size`：批次大小<br>
	`timesteps`：单个序列的长度<br>
	`output_features`：输出特征<br>


### 6.2.2 理解 LSTM 层和 GRU 层
### 6.2.3 Keras 中一个 LSTM 的具体例子
### 6.2.4 小结

## 6.3 循环神经网络的高级用法
### 6.3.1 温度预测问题
### 6.3.2 准备数据
### 6.3.3 一种基于常识的、非机器学习的基准方法
### 6.3.4 一种基本的机器学习方法
### 6.3.5 第一个循环网络基准
### 6.3.6 使用循环dropout来降低过拟合
### 6.3.7 循环层堆叠
### 6.3.8 使用双向RNN
### 6.3.9 更多尝试
### 6.3.10 小结

## 6.4 用卷积神经网络处理序列
### 6.4.1 理解序列数据的一维卷积
### 6.4.2 序列数据的一维池化
### 6.4.3 实现一维卷积神经网络
### 6.4.4 结合CNN和RNN来处理长序列
### 6.4.5 小结

## 本章总结

