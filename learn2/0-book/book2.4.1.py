#coding=utf-8

'''

1.神经网络的“引擎”:基于梯度优化
    所谓引擎，即牵引前进的东西，利用梯度优化

    数据输入网络，得到一个输出y_pred，比较正确的y与y_pred之间的差距，即损失loss，而这个损失loss是关于权重的函数loss=f(w),于是学习的过程就变成了找到最合适的权重w，使损失loss最小，
        1.先前想的是，求函数的最小点，可以先求其导函数loss=f'(w),极点肯定会出现在导函数值为0的地方，根据牛顿莱布滋公式，求导函数的积分，积分最小点就是函数loss=f(w)的最小点，从而得到能使损失loss最小的权重w
        2.通常用随机梯度下降法(SGD)，(梯度：张量的导数),先求其导函数，然后设定步长，将权重w向导函数的相反方向移动一个步长，更新w，由于是向导函数的相反方向，所有比如会得到更小的损失loss，由此循环♻️ 优化w
        此外，SGD还有很多变种:带动量的SGD,Adagrad,RMSProp等
    计算损失相对与网络权重的梯度，即反向传播

2.反向传播算法
    链式法则：( f(g(x)) )' = f'(g(x)) * g'(x)
    要求嵌套函数的导数，可以用链式法则分解成分解函数的导数来求得
    现在以及未来数年，人们将使用能够进行符号微分(symbolic differentiation)的现代框架来实现神经网络，比如tensorflow，也就是说，给定一个运算链，并且已知每个运算的导数，这个框架就可以利用链式法则来计算这个计算链的梯度函数
    

'''
