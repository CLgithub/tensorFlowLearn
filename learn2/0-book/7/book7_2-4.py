# coding=utf-8

# ä½¿ç”¨ä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œå¤„ç†IMDBæƒ…æ„Ÿåˆ†ç±»é—®é¢˜

import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt

from keras.datasets import imdb
from keras.preprocessing import sequence
from keras import layers,models
from keras.optimizers import RMSprop
from keras.callbacks import TensorBoard

# import tensorflow as tf
# # é…ç½®gpuè®­ç»ƒæ—¶å†…å­˜åˆ†é…ï¼Œåº”è¯¥å•ç‹¬å­¦ä¹ gpuèµ„æºç®¡ç†ï¼Œåˆç†åˆ†é…gpuèµ„æºï¼Œæ‰èƒ½æ›´å¥½çš„åˆ©ç”¨ï¼Œtensorflowè¿˜æ²¡èƒ½åœ¨å·¥å…·å±‚å¤„ç†è¿™é—®é¢˜ï¼Œæ‰€ä»¥æ‰å¿…é¡»åœ¨ä»£ç ä¸­è¿›è¡Œé…ç½®
# config = tf.ConfigProto(log_device_placement=False)    # æ˜¯å¦æ‰“å°è®¾å¤‡åˆ†é…æ—¥å¿—
# config.gpu_options.per_process_gpu_memory_fraction=0.5 # è®¾ç½®æ¯ä¸ªgpuåº”è¯¥æ‹¿å‡ºå¤šå°‘å®¹é‡ç»™è¿›ç¨‹ä½¿ç”¨
# config.operation_timeout_in_ms=15000   # terminate on long hangs
# sess = tf.InteractiveSession("", config=config)
# init = tf.global_variables_initializer()
# sess.run(init)


max_features = 10000
maxlen = 500  # åºåˆ—é•¿åº¦
batch_size = 32 # åºåˆ—ä¸ªæ•°

(train_x, train_y),(test_x, test_y) = imdb.load_data(num_words=max_features)

# print(train_x[0])

# å°†æ¯ä¸ªåºåˆ—è¿›è¡Œåè½¬
# train_x = [x[::-1] for x in train_x]
# test_x = [x[::-1] for x in test_x]

train_x = sequence.pad_sequences(train_x, maxlen=maxlen)
test_x = sequence.pad_sequences(test_x, maxlen=maxlen)

def getModel():
    model = models.Sequential()
    model.add(layers.Embedding(max_features, 128, input_length=maxlen, name='embed'))   # åµŒå…¥å±‚,åºåˆ—å‘é‡å­—å…¸(10000,128)
    model.add(layers.Conv1D(32, 7, activation='relu'))  # æ·»åŠ ä¸€ä¸ª1Då·ç§¯å±‚ï¼Œå·ç§¯çª—å£é•¿åº¦7ï¼Œ32ä¸ªç‰¹å¾
    model.add(layers.MaxPooling1D(5))                   # æ·»åŠ ä¸€ä¸ª1Dæ± åŒ–å±‚ï¼Œæ± åŒ–çª—å£é•¿åº¦5
    model.add(layers.Conv1D(32, 7, activation='relu')) 
    model.add(layers.GlobalMaxPooling1D())              # 
    # model.add(layers.Dropout(0.5))  # å¢åŠ ä¸€ä¸ªdropoutå±‚ï¼Œå‡å°è¿‡æ‹Ÿåˆ
    # model.add(layers.Dense(10, activation='relu'))
    # model.add(layers.Dense(1))
    model.add(layers.Dense(1, activation='sigmoid'))

    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['acc'] )
    # model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'] )

    return model

def getCallBacks():
    callbacks = [
        TensorBoard(    # å®ä¾‹åŒ–tensorBoardå¯è§†åŒ–å›è°ƒå‡½æ•°
            log_dir = 'my_log_dir_7_2-4',   # æ—¥å¿—æ–‡ä»¶ä¿å­˜ä½ç½®
            histogram_freq = 1,             # æ²¡å¤šå°‘è½®ä¹‹åè®°å½•æ¿€æ´»ç›´æ–¹å›¾ğŸ“Š
            embeddings_freq = 1,             # æ²¡å¤šå°‘è½®ä¹‹åè®°å½•åµŒå…¥æ•°æ®ï¼Œå³åµŒå…¥å‘é‡å½¢çŠ¶
            embeddings_data = train_x[:100].astype("float32")   # ä¸åŠ ä¸Šä¼šæŠ¥ï¼šValueErrorï¼šTo visualize embeddings, embeddings_data must be provided.
        )
    ]
    return callbacks

def run(model):
    history = model.fit(
        train_x,train_y,
        epochs=10,
        batch_size=128,
        validation_split=0.2,
        callbacks=getCallBacks()
    )
    return history


def show2(t_loss,t_acc,v_loss,v_acc):
    epochs=range(1, len(t_loss)+1)
    plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plt.plot(epochs, t_loss, 'b', label='t_loss')
    plt.plot(epochs, v_loss, 'r', label='v_loss')
    plt.ylim([0,1])
    plt.title('loss')
    plt.legend()
    plt.subplot(1,2,2)
    plt.plot(epochs, t_acc, 'b', label='t_acc')
    plt.plot(epochs, v_acc, 'r', label='v_acc')
    plt.ylim([0,1])
    plt.title('acc')
    plt.legend()
    plt.show()


if __name__ == '__main__':
    model=getModel()
    history=run(model)
    t_loss=history.history['loss']
    t_acc=history.history['acc']
    v_loss=history.history['val_loss']
    v_acc=history.history['val_acc']
    show2(t_loss,t_acc,v_loss,v_acc)


