# 八、生成式深度学习
本章包括以下内容:

* 使用LSTM生成文本
* 使用DeepDream
* 使用神经风格迁移
* 变分自编码
* 了解生成式对抗网络

机器学习模型能够对图像、音乐和故事的统计**潜在空间(latent space)**进行学习，然后从这个空间中**采样(sample)**，创造出与模型在训练数据中所见到的艺术作品具有相似特征的新作品。

它从一种与我们的经验完全不同的经验中进行学习

作为人类旁观者，只能靠我们的解释才能对模型生成的内容赋予意义。

👻：生成的内容本身没有什么意义，人类的解释才能对内容赋予意义

👻：和自监督学习有何关系

## 8.1 使用LSTM生成文本
本节将会探讨如何将循环神经网络用于生成序列数据
### 8.1.1 生成式循环网络简史
2013年Alex Graves 利用笔触位置的时间序列将循环混合密度网络应用于生成 类似人类的手写笔迹，有人认为这是一个转折点。

“序列数据生成是计算机所做的最接近于做梦的事情。”

从那以后，循环神经网络已被成功应用于音乐生成、对话生成、图像生成、语音合成和分 子设计。它甚至还被用于制作电影剧本，然后由真人演员来表演。

### 8.1.2 如何生成序列数据
通用方法：使用前面的标记 作为 **输入**，训练一个网络，来**预测**序列中接下来的**一个或多个标记**<br>
例如：给定输入 “the cat is on the ma”，训练网络来预测下一个字符 "t"

**语言模型(language model)**：给定标记，能够对下一个标记的概率进行建模的任何网络

**潜在空间(latent space)**：语言的统计结构

**整体流程**：向模型中输入一个初始文本字符串，要求模型生成下一个字符或下一个或多个单词，然后将生成的输出添加到输入数据中，多重重复这一过程
<center>![](./images/8.1-1.png)</center>

### 8.1.3 采样策略的重要性

* **贪婪采样(greedy sampling)**：贪婪抽取，从模型预测的下一个字符的概率分布中，抽取概率最大的
* **随机采样(stochastic sampling)**：随机抽取，从模型预测的下一个字符的概率分布中，随机抽取
* 贪婪抽取最具有更加可预测的结构，随机抽取最更有创造性的机构，控制偏向哪边的参数 设定为temperature，取汁0～1，
`temperature=0 #贪婪抽取`
`temperature=1 #随机抽取`

[对于不同的 softmax 温度，对概率分布进行重新加权x](./images/book8_1-1.py)

### 8.1.4 实现字符级的LSTM文本生成
[实现字符级的LSTM文本生成](./book8_1-2.py)

温度temperature越高，越随机

该模型所做的只是从一个统计模型中对数据进行采样，这个模型是关于**字符先后顺序**的模型；信息的**内容**与信息**编码的统计结构**是有区别的

### 8.1.5 小结

* 我们可以生成离散的序列数据，其方法是:给定前面的标记，训练一个模型来预测接下来的一个或多个标记。
* 对于文本来说，这种模型叫作**语言模型**。它可以是单词级的，也可以是字符级的。
* 对下一个标记进行采样，需要在坚持模型的判断与引入随机性之间寻找平衡。
* 处理这个问题的一种方法是使用 softmax 温度。一定要尝试多种不同的温度，以找到合适的那一个。

## 8.2 DeepDream
### 8.2.1 用Keras实现DeepDream
[用Keras实现DeepDream](./book8_2-1.py)
### 8.2.2 小结

## 8.3 神经风格迁移
### 8.3.1 内容损失
### 8.3.2 风格损失
### 8.3.3 用Keras实现神经风格迁移
### 8.3.4 小结

## 8.4 用变分自编码器生成图像
### 8.4.1 从图像的潜在空间中采样
### 8.4.2 图像编辑的概念向量
### 8.4.3 变分自编码器
### 8.4.4 小结

## 8.5 生成式对抗网络简介
### 8.5.1 GAN的简要实现流程
### 8.5.2 大量技巧
### 8.5.3 生成器
### 8.5.4 判别器
### 8.5.5 对抗网络
### 8.5.6 如何训练DCGAN
### 8.5.7 小结

## 本章总结