# 三、神经网络入门

## 3.1 神经网络剖析
* <b>层</b>，多个层组合成<b>网络</b>（或<b>模型</b>）
* <b>输入数据</b>和对应的<b>目标</b>
* <b>损失函数</b>：训练过程中需要将其最小化
* <b>优化器</b>：决定如何基于损失函数对网络进行更新，它的执行是随机梯度下降（<b>SGD</b>）的某个变体
* <b>衡量标尺</b>：

## 3.4.7 小结（page=59）
下面是你应该从[这个例子](https://github.com/CLgithub/tensorFlowLearn/blob/master/learn2/0-book/3/book3.4.py)（影评二分类）中学到的要点：

* 通常需要对原始数据进行大量预处理，以便将其转换为张量输入到神经网络中。单词序列可以编码为二进制向量，但也有其他编码方式。
* 带有relu激活的Dense层堆叠，可以解决很多问题（包括情感分类），你可能会经常用到这种模型。
* 对于二分类问题（两个输出类别），网络的最后一层应该是只有一个单元并使用sigmoid激活的Dense层，网络输出应该是0～1范围内的标量，表示概率值。
* 对于二分类问题的sigmoid标量输出，你应该使用binary\_crossentropy损失函数。
* 无论你的问题是什么，rmsprop优化器通常都是足够好的选择。
* 随着神经网络在训练数据上的表现越来越好，模型最终会过拟合，并在前所未见的数据上得到越来越差的结果。一定要一直监控模型在训练集之外的数据上的性能。

## 3.5.9 小结（page=66）
下面是你应该从[这个例子](https://github.com/CLgithub/tensorFlowLearn/blob/master/learn2/0-book/3/book3.5.py)（新闻多分类问题）中学到的要点：

* 如果要对N个类别的数据点进行分类，网络的最后一层应该是大小为N的Dense层。
* 对于单标签、多分类问题，网络的最后一层应该使用softmax激活函数，这样可以输出在N个输出类别上的概率分布。
* 这种问题的损失函数几乎总是应该使用分类交叉熵。它将网络输出的概率分布与目标的真是分布之间的距离最小化。
* 处理多分类问题的标签有两种方法：
	* 通过分类编码（也叫one-hot编码）对标签进行编码，然后使用categorical\_crossentropy作为损失函数。
	* 将标签编码为整数，然后使用sparse\_categorical\_crossentropy损失函数。
* 如果你需要将数据划分到许多分类中，应该避免使用太小的中间层，以免在网络中造成信息瓶颈。

## 3.6.5 小结（page=72）
下面是你应该从[这个例子](https://github.com/CLgithub/tensorFlowLearn/blob/master/learn2/0-book/3/book3.6.py)（预测房价回归问题）中学到的要点：

* 何为回归问题：分类问题标签是离散的，回归问题的标签是连续的
* 回归问题使用的损失函数与分类问题不同。回归常用的损失函数是均方误差（mean squared error），分类问题使用的损失函数是分类交叉熵，二分类用categorical\_crossentropy，多分类问题用sparse\_categorical\_crossentropy
* 同样，回归问题使用的评估指标与分类问题不同，显而易见，精度的概念不适用于回归问题。常见的回归指标是平均绝对误差（MAE）
* 如果输入数据的特征具有不同的取值范围，应该先进行预处理，对每个特征单独进行缩放。
* 如果可用的数据很少，使用K折验证可以可靠地评估模型。
* 如果可用的训练数据很少，最好使用隐藏层较少（通常只有一到两个）的小型网络，以避免严重的过拟合

## 本章小结

* 现在你可以处理关于向量数据最常见的机器学习任务了:二分类问题、多分类问题和标 量回归问题。前面三节的“小结”总结了你从这些任务中学到的要点。
* 在将原始数据输入神经网络之前，通常需要对其进行预处理。
* 如果数据特征具有不同的取值范围，那么需要进行预处理，将每个特征单独缩放。
* 随着训练的进行，神经网络最终会过拟合，并在前所未见的数据上得到更差的结果。
* 如果训练数据不是很多，应该使用只有一两个隐藏层的小型网络，以避免严重的过拟合。
* 如果数据被分为多个类别，那么中间层过小可能会导致信息瓶颈。
* 回归问题使用的损失函数和评估指标都与分类问题不同。
* 如果要处理的数据很少，K 折验证有助于可靠地评估模型。

## 本章自我总结

* 激活函数
	* relu(rectified linear unit整流线性单元)：函数将所有负值归零
	* sigmoid：而sigmoid函数则将任意值“压缩”到 [0, 1] 区间内，其输出值可以看作概率值
* 编译选择

——|二分类|多分类|回归
:--|:--:|:--:|:--:
<b>损失函数</b>|binary\_crossentropy<br>(二元交叉熵)|categorical_crossentropy<br>(分类交叉熵,one-hot分类编码)<br>sparse\_categorical\_crossentropy<br>(稀少分类交叉熵,整数张量)|mse<br>(均方误差mean squared error)
<b>优化器</b>|rmsprop(SGD的变体)|rmsprop|rmsprop
<b>衡量指标</b>|accuracy|accuracy|mea<br>(平均绝对误差mean absolute error)

